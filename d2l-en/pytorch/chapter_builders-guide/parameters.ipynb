{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7443df46",
   "metadata": {
    "origin_pos": 1
   },
   "source": [
    "# **Parameter Management**\n",
    "\n",
    "- **Once an architecture and hyperparameters are set**, the next step is the **training loop**:\n",
    "  - The goal is to **find parameter values** that **minimize the loss function**.\n",
    "  - After training, parameters are **needed for future predictions**.\n",
    "\n",
    "- **Why extract parameters?**\n",
    "  - To **reuse them** in another context.\n",
    "  - To **save the model** for execution in other software.\n",
    "  - To **examine parameters** for gaining scientific understanding.\n",
    "\n",
    "- **Most of the time**, deep learning frameworks handle parameter declaration and manipulation.\n",
    "  - However, in **non-standard architectures**, manual parameter management is sometimes required.\n",
    "\n",
    "- **Topics covered in this section**:\n",
    "  - **Accessing parameters** for debugging, diagnostics, and visualizations.\n",
    "  - **Sharing parameters** across different model components.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41cbf7e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:27:20.807089Z",
     "iopub.status.busy": "2023-08-18T19:27:20.806426Z",
     "iopub.status.idle": "2023-08-18T19:27:22.457089Z",
     "shell.execute_reply": "2023-08-18T19:27:22.456154Z"
    },
    "origin_pos": 3,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293084ba",
   "metadata": {
    "origin_pos": 6
   },
   "source": [
    "- **We start by focusing on an MLP with one hidden layer.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9aa0461f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:27:22.461279Z",
     "iopub.status.busy": "2023-08-18T19:27:22.460607Z",
     "iopub.status.idle": "2023-08-18T19:27:22.494399Z",
     "shell.execute_reply": "2023-08-18T19:27:22.493545Z"
    },
    "origin_pos": 8,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(nn.LazyLinear(8),\n",
    "                    nn.ReLU(),\n",
    "                    nn.LazyLinear(1))\n",
    "\n",
    "X = torch.rand(size=(2, 4))\n",
    "net(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0bb094",
   "metadata": {
    "origin_pos": 11
   },
   "source": [
    "## **Parameter Access**\n",
    ":label:`subsec_param-access`\n",
    "\n",
    "- We begin by exploring **how to access parameters** from models.\n",
    "- This applies to **models we have already encountered**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e03323",
   "metadata": {
    "origin_pos": 12,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "- When a model is defined using the **`Sequential` class**:\n",
    "  - Any **layer** can be accessed by **indexing into the model**, just like a list.\n",
    "  - Each layerâ€™s **parameters** are stored in its **attributes**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f41ac20",
   "metadata": {
    "origin_pos": 14
   },
   "source": [
    "- We can inspect the parameters of the second fully connected layer as follows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c6fdb60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:27:22.497996Z",
     "iopub.status.busy": "2023-08-18T19:27:22.497442Z",
     "iopub.status.idle": "2023-08-18T19:27:22.504291Z",
     "shell.execute_reply": "2023-08-18T19:27:22.503521Z"
    },
    "origin_pos": 16,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[-0.1649,  0.0605,  0.1694, -0.2524,  0.3526, -0.3414, -0.2322,  0.0822]])),\n",
       "             ('bias', tensor([0.0709]))])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[2].state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9e6e49",
   "metadata": {
    "origin_pos": 19
   },
   "source": [
    "- A **fully connected layer** contains **two parameters**:\n",
    "  - **Weights**.\n",
    "  - **Biases**.\n",
    "\n",
    "### **Targeted Parameters**\n",
    "\n",
    "- Each parameter is represented as an **instance of the parameter class**.\n",
    "- To manipulate parameters, we must **access their underlying numerical values**.\n",
    "\n",
    "- **Ways to access parameter values**:\n",
    "  - Some methods are **simpler**.\n",
    "  - Others are **more general**.\n",
    "\n",
    "- **Example**:\n",
    "  - The following code extracts the **bias** from the second neural network layer.\n",
    "  - It first **retrieves a parameter class instance**.\n",
    "  - Then, it **further accesses the parameter's value**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba2da7b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:27:22.507849Z",
     "iopub.status.busy": "2023-08-18T19:27:22.507181Z",
     "iopub.status.idle": "2023-08-18T19:27:22.513236Z",
     "shell.execute_reply": "2023-08-18T19:27:22.512406Z"
    },
    "origin_pos": 21,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.nn.parameter.Parameter, tensor([0.0709]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(net[2].bias), net[2].bias.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c10cf6c",
   "metadata": {
    "origin_pos": 24,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "- **Parameters are complex objects**, containing:\n",
    "  - **Values** (numerical weights or biases).\n",
    "  - **Gradients** (used for optimization).\n",
    "  - **Additional metadata**.\n",
    "\n",
    "- **Explicitly requesting parameter values**:\n",
    "  - Since parameters store more than just values, we must **explicitly request** the numerical value.\n",
    "\n",
    "- **Accessing gradients**:\n",
    "  - Each parameter also provides access to its **gradient**.\n",
    "  - If **backpropagation has not been invoked**, the gradient remains in its **initial state**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c5f0ae9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:27:22.516723Z",
     "iopub.status.busy": "2023-08-18T19:27:22.516170Z",
     "iopub.status.idle": "2023-08-18T19:27:22.521606Z",
     "shell.execute_reply": "2023-08-18T19:27:22.520790Z"
    },
    "origin_pos": 27,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[2].weight.grad == None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d744bc",
   "metadata": {
    "origin_pos": 28
   },
   "source": [
    "### **All Parameters at Once**\n",
    "\n",
    "- **Accessing parameters one by one** can be **tedious**, especially when:\n",
    "  - The model is **large**.\n",
    "  - The architecture is **nested with multiple sub-modules**.\n",
    "  \n",
    "- **Challenges with complex models**:\n",
    "  - Requires **recursively traversing** the entire module tree.\n",
    "  - Extracting parameters manually can be **cumbersome**.\n",
    "\n",
    "- **Solution**:\n",
    "  - We can **access all parameters at once**.\n",
    "  - The following example demonstrates how to **retrieve parameters from all layers** efficiently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dab1b4b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:27:22.525019Z",
     "iopub.status.busy": "2023-08-18T19:27:22.524380Z",
     "iopub.status.idle": "2023-08-18T19:27:22.530002Z",
     "shell.execute_reply": "2023-08-18T19:27:22.529195Z"
    },
    "origin_pos": 30,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0.weight', torch.Size([8, 4])),\n",
       " ('0.bias', torch.Size([8])),\n",
       " ('2.weight', torch.Size([1, 8])),\n",
       " ('2.bias', torch.Size([1]))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(name, param.shape) for name, param in net.named_parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd29a2e",
   "metadata": {
    "origin_pos": 33
   },
   "source": [
    "## **Tied Parameters**\n",
    "\n",
    "- **Sharing parameters across multiple layers**:\n",
    "  - Often, we want to **reuse the same parameters** in different layers.\n",
    "  - This helps **reduce memory usage** and **maintain consistency** across layers.\n",
    "\n",
    "- **How to achieve this?**\n",
    "  1. **Allocate a fully connected layer**.\n",
    "  2. **Use its parameters** to set those of **another layer**.\n",
    "\n",
    "- **Key implementation detail**:\n",
    "  - Before accessing the parameters, we must first **run forward propagation** using `net(X)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b706636",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:27:22.533421Z",
     "iopub.status.busy": "2023-08-18T19:27:22.532786Z",
     "iopub.status.idle": "2023-08-18T19:27:22.541856Z",
     "shell.execute_reply": "2023-08-18T19:27:22.541011Z"
    },
    "origin_pos": 35,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True, True, True, True, True])\n",
      "tensor([True, True, True, True, True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "# We need to give the shared layer a name so that we can refer to its\n",
    "# parameters\n",
    "shared = nn.LazyLinear(8)\n",
    "net = nn.Sequential(nn.LazyLinear(8), nn.ReLU(),\n",
    "                    shared, nn.ReLU(),\n",
    "                    shared, nn.ReLU(),\n",
    "                    nn.LazyLinear(1))\n",
    "\n",
    "net(X)\n",
    "# Check whether the parameters are the same\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])\n",
    "net[2].weight.data[0, 0] = 100\n",
    "# Make sure that they are actually the same object rather than just having the\n",
    "# same value\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec93f84",
   "metadata": {
    "origin_pos": 38
   },
   "source": [
    "- **Parameter tying in layers**:\n",
    "  - In this example, the **parameters of the second and third layers are tied**.\n",
    "  - They are **not just equal** but are actually **represented by the same exact tensor**.\n",
    "\n",
    "- **Implication of tied parameters**:\n",
    "  - **Modifying one parameter** will **automatically update** the other.\n",
    "  - This ensures **synchronization** between the layers using shared parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7b800f",
   "metadata": {
    "origin_pos": 39,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "- **What happens to gradients when parameters are tied?**\n",
    "  - Since model parameters **store gradients**, tying parameters affects backpropagation.\n",
    "\n",
    "- **Gradient behavior in tied parameters**:\n",
    "  - During **backpropagation**, the gradients of:\n",
    "    - The **second hidden layer**.\n",
    "    - The **third hidden layer**.\n",
    "  - Are **added together** before updating the shared parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f0f0ad",
   "metadata": {
    "origin_pos": 40
   },
   "source": [
    "## **Summary**\n",
    "\n",
    "- We have several ways of **accessing model parameters**.\n",
    "- We have several ways of **tying model parameters**.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "required_libs": []
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
