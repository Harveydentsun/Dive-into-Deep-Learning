{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0542ba11-6334-4fc8-875a-e7467f191b39",
   "metadata": {
    "origin_pos": 1
   },
   "source": [
    "# Layers and Modules\n",
    ":label:`sec_model_construction`\n",
    "\n",
    "- When we first introduced neural networks, we focused on linear models with a single output.\n",
    "  - The entire model consists of just a single neuron.\n",
    "  - A single neuron:\n",
    "    - Takes some set of inputs.\n",
    "    - Generates a corresponding scalar output.\n",
    "    - Has a set of associated parameters that can be updated to optimize some objective function.\n",
    "- When dealing with networks with multiple outputs, we leveraged vectorized arithmetic to characterize an entire layer of neurons.\n",
    "  - Like individual neurons, layers:\n",
    "    - Take a set of inputs.\n",
    "    - Generate corresponding outputs.\n",
    "    - Are described by a set of tunable parameters.\n",
    "- In softmax regression, a single layer was itself the model.\n",
    "- In multi-layer perceptrons (MLPs), the model retains this basic structure.\n",
    "\n",
    "- In MLPs, both the entire model and its constituent layers share a similar structure:\n",
    "  - The entire model:\n",
    "    - Takes in raw inputs (features).\n",
    "    - Generates outputs (predictions).\n",
    "    - Possesses parameters (combined from all constituent layers).\n",
    "  - Each individual layer:\n",
    "    - Ingests inputs from the previous layer.\n",
    "    - Generates outputs for the subsequent layer.\n",
    "    - Has tunable parameters updated according to the signal flowing backward.\n",
    "\n",
    "- Neurons, layers, and models provide useful abstractions, but sometimes, we need intermediate components:\n",
    "  - Example: The ResNet-152 architecture (widely used in computer vision) consists of hundreds of layers.\n",
    "    - Layers form repeating patterns of *groups of layers*.\n",
    "    - Implementing such networks one layer at a time is tedious.\n",
    "    - The ResNet architecture won the 2015 ImageNet and COCO competitions for recognition and detection :cite:`He.Zhang.Ren.ea.2016`.\n",
    "    - Similar architectures are common in natural language processing and speech processing.\n",
    "\n",
    "- To implement complex networks efficiently, we introduce the concept of a neural network *module*:\n",
    "  - A module can describe:\n",
    "    - A single layer.\n",
    "    - A component consisting of multiple layers.\n",
    "    - The entire model itself.\n",
    "  - Benefits of the module abstraction:\n",
    "    - Modules can be combined into larger structures recursively.\n",
    "    - Enables writing compact yet powerful code for complex networks.\n",
    "    - Illustrated in :numref:`fig_blocks`.\n",
    "\n",
    "![Multiple layers are combined into modules, forming repeating patterns of larger models.](../img/blocks.svg)\n",
    ":label:`fig_blocks`\n",
    "\n",
    "- From a programming perspective, a module is represented by a *class*:\n",
    "  - Any subclass must define:\n",
    "    - A forward propagation method to transform input into output.\n",
    "    - Storage for any necessary parameters (some modules may not require parameters).\n",
    "  - The module must also support backpropagation for gradient calculations.\n",
    "  - Auto differentiation (introduced in :numref:`sec_autograd`) simplifies this process:\n",
    "    - We only need to define parameters and the forward propagation method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b911ed7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:31:38.169811Z",
     "iopub.status.busy": "2023-08-18T19:31:38.169219Z",
     "iopub.status.idle": "2023-08-18T19:31:40.246403Z",
     "shell.execute_reply": "2023-08-18T19:31:40.245375Z"
    },
    "origin_pos": 3,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80abfaa",
   "metadata": {
    "origin_pos": 6
   },
   "source": [
    "- To begin, we revisit the code that we used to implement MLPs (:numref:`sec_mlp`).\n",
    "- The following code generates a neural network with:\n",
    "  - One fully connected hidden layer:\n",
    "    - Contains **256 units**.\n",
    "    - Uses **ReLU activation**.\n",
    "  - A fully connected output layer:\n",
    "    - Contains **10 units**.\n",
    "    - Has **no activation function**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7df830d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:31:40.251527Z",
     "iopub.status.busy": "2023-08-18T19:31:40.250671Z",
     "iopub.status.idle": "2023-08-18T19:31:40.284734Z",
     "shell.execute_reply": "2023-08-18T19:31:40.283757Z"
    },
    "origin_pos": 8,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(nn.LazyLinear(256), nn.ReLU(), nn.LazyLinear(10))\n",
    "\n",
    "X = torch.rand(2, 20)\n",
    "net(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe16ebaf",
   "metadata": {
    "origin_pos": 12,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "- In this example, we constructed our model by instantiating an `nn.Sequential`:\n",
    "  - Layers are passed as arguments in the order they should be executed.\n",
    "- **`nn.Sequential` defines a special kind of `Module` in PyTorch**:\n",
    "  - It maintains an **ordered list** of constituent `Module`s.\n",
    "  - Each fully connected layer is an instance of the `Linear` class.\n",
    "    - `Linear` itself is a subclass of `Module`.\n",
    "- The `forward` propagation method is straightforward:\n",
    "  - It **chains each module** in the list together.\n",
    "  - The output of one module is **passed as input** to the next module.\n",
    "- Until now, we have been invoking models using `net(X)`:\n",
    "  - This is shorthand for **`net.__call__(X)`**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93754877",
   "metadata": {
    "origin_pos": 14
   },
   "source": [
    "## **A Custom Module**\n",
    "\n",
    "- The easiest way to understand how a module works is to implement one ourselves.\n",
    "- Before doing so, we summarize the basic functionality that each module must provide:\n",
    "  1. **Ingest input data** as arguments to its forward propagation method.\n",
    "  2. **Generate an output** by returning a value from the forward propagation method.\n",
    "     - The output may have a **different shape** from the input.\n",
    "     - Example: In our previous model, the first fully connected layer:\n",
    "       - Takes an input of arbitrary dimension.\n",
    "       - Returns an output of dimension **256**.\n",
    "  3. **Calculate the gradient** of its output with respect to its input.\n",
    "     - This is accessible via its **backpropagation method**.\n",
    "     - Typically, this happens **automatically**.\n",
    "  4. **Store and provide access** to necessary parameters for forward propagation computation.\n",
    "  5. **Initialize model parameters** as needed.\n",
    "\n",
    "- The following snippet implements a **custom module**:\n",
    "  - Corresponds to an **MLP** with:\n",
    "    - One hidden layer of **256 hidden units**.\n",
    "    - A **10-dimensional output layer**.\n",
    "  - The `MLP` class **inherits** from the base module class.\n",
    "  - We rely heavily on **parent class methods**:\n",
    "    - We only need to define:\n",
    "      - The **constructor** (`__init__` method in Python).\n",
    "      - The **forward propagation** method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c5f010e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:31:40.289115Z",
     "iopub.status.busy": "2023-08-18T19:31:40.288828Z",
     "iopub.status.idle": "2023-08-18T19:31:40.295756Z",
     "shell.execute_reply": "2023-08-18T19:31:40.294461Z"
    },
    "origin_pos": 16,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        # Call the constructor of the parent class nn.Module to perform\n",
    "        # the necessary initialization\n",
    "        super().__init__()\n",
    "        self.hidden = nn.LazyLinear(256)\n",
    "        self.out = nn.LazyLinear(10)\n",
    "\n",
    "    # Define the forward propagation of the model, that is, how to return the\n",
    "    # required model output based on the input X\n",
    "    def forward(self, X):\n",
    "        return self.out(F.relu(self.hidden(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7eaced",
   "metadata": {
    "origin_pos": 19
   },
   "source": [
    "- Let's first focus on the **forward propagation method**:\n",
    "  - Takes `X` as input.\n",
    "  - Calculates the **hidden representation** with the activation function applied.\n",
    "  - Outputs **logits**.\n",
    "\n",
    "- In this `MLP` implementation:\n",
    "  - Both layers are **instance variables**.\n",
    "  - Why is this reasonable?\n",
    "    - Imagine two MLP instances: `net1` and `net2`.\n",
    "    - If trained on different data, they should represent **different learned models**.\n",
    "\n",
    "- **Instantiation of MLP's layers** occurs in the constructor:\n",
    "  - These layers are subsequently **invoked in each call** to the forward propagation method.\n",
    "\n",
    "- **Key implementation details**:\n",
    "  1. The customized `__init__` method:\n",
    "     - Calls the parent class's `__init__` method via **`super().__init__()`**.\n",
    "     - Avoids repeating boilerplate code applicable to most modules.\n",
    "  2. Two fully connected layers are instantiated and assigned to:\n",
    "     - `self.hidden` → Represents the **hidden layer**.\n",
    "     - `self.out` → Represents the **output layer**.\n",
    "  3. No need to implement:\n",
    "     - **Backpropagation method**.\n",
    "     - **Parameter initialization**.\n",
    "     - The system **automatically handles these** unless we define a custom layer.\n",
    "\n",
    "- Let's try this out!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c8301dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:31:40.300597Z",
     "iopub.status.busy": "2023-08-18T19:31:40.300120Z",
     "iopub.status.idle": "2023-08-18T19:31:40.308051Z",
     "shell.execute_reply": "2023-08-18T19:31:40.307090Z"
    },
    "origin_pos": 20,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MLP()\n",
    "net(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cef5ea",
   "metadata": {
    "origin_pos": 22
   },
   "source": [
    "- A key virtue of the **module abstraction** is its **versatility**:\n",
    "  - We can **subclass a module** to create:\n",
    "    - **Layers** (e.g., fully connected layer class).\n",
    "    - **Entire models** (e.g., `MLP` class).\n",
    "    - **Intermediate complexity components**.\n",
    "  - This versatility will be **exploited throughout the coming chapters**, such as:\n",
    "    - **Convolutional neural networks** (CNNs).\n",
    "\n",
    "## **The Sequential Module**\n",
    ":label:`subsec_model-construction-sequential`\n",
    "\n",
    "- We now take a **closer look** at how the `Sequential` class works.\n",
    "- **`Sequential` is designed to daisy-chain other modules together**.\n",
    "\n",
    "- To build a simplified version (`MySequential`), we need to define two key methods:\n",
    "  1. **A method to append modules** one by one into a list.\n",
    "  2. **A forward propagation method**:\n",
    "     - Passes an input through the chain of modules.\n",
    "     - Follows the **same order as they were appended**.\n",
    "\n",
    "- The following `MySequential` class provides the **same functionality** as the default `Sequential` class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09b7f913",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:31:40.312685Z",
     "iopub.status.busy": "2023-08-18T19:31:40.312400Z",
     "iopub.status.idle": "2023-08-18T19:31:40.318061Z",
     "shell.execute_reply": "2023-08-18T19:31:40.317031Z"
    },
    "origin_pos": 24,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "class MySequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        for idx, module in enumerate(args):\n",
    "            self.add_module(str(idx), module)\n",
    "\n",
    "    def forward(self, X):\n",
    "        for module in self.children():\n",
    "            X = module(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6a2da4",
   "metadata": {
    "origin_pos": 28,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "- In the `__init__` method:\n",
    "  - Every module is added by calling the **`add_modules` method**.\n",
    "  - These added modules can later be accessed via the **`children` method**.\n",
    "\n",
    "- **Why is this important?**\n",
    "  - The system keeps track of the **added modules**.\n",
    "  - It ensures that each module's **parameters are properly initialized**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008229d2",
   "metadata": {
    "origin_pos": 29
   },
   "source": [
    "- When `MySequential`'s **forward propagation method** is invoked:\n",
    "  - Each **added module** is executed **in the order they were added**.\n",
    "\n",
    "- We can now **reimplement an MLP** using our `MySequential` class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b8d5d6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:31:40.323023Z",
     "iopub.status.busy": "2023-08-18T19:31:40.322454Z",
     "iopub.status.idle": "2023-08-18T19:31:40.330187Z",
     "shell.execute_reply": "2023-08-18T19:31:40.329340Z"
    },
    "origin_pos": 31,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MySequential(nn.LazyLinear(256), nn.ReLU(), nn.LazyLinear(10))\n",
    "net(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98fc5fe",
   "metadata": {
    "origin_pos": 34
   },
   "source": [
    "- The use of `MySequential` is **identical** to the code we previously wrote for the `Sequential` class.\n",
    "  - As described in :numref:`sec_mlp`.\n",
    "\n",
    "## **Executing Code in the Forward Propagation Method**\n",
    "\n",
    "- The `Sequential` class makes **model construction easy**:\n",
    "  - Allows assembling architectures **without defining a custom class**.\n",
    "- However, **not all architectures** are simple daisy chains:\n",
    "  - When greater **flexibility** is required, we define **custom blocks**.\n",
    "\n",
    "- **Why define custom blocks?**\n",
    "  1. **Executing Python control flow** within the forward propagation method.\n",
    "  2. **Performing arbitrary mathematical operations** beyond predefined neural network layers.\n",
    "\n",
    "- **Network operations so far**:\n",
    "  - Have **acted upon network activations** and **trainable parameters**.\n",
    "  - But sometimes, we need **constant parameters** that:\n",
    "    - Are **not derived from previous layers**.\n",
    "    - Are **not updated during optimization**.\n",
    "\n",
    "- **Example: A layer calculating**  \n",
    "  \\[\n",
    "  f(\\mathbf{x},\\mathbf{w}) = c \\cdot \\mathbf{w}^\\top \\mathbf{x}\n",
    "  \\]\n",
    "  - Where:\n",
    "    - **$\\mathbf{x}$** is the input.\n",
    "    - **$\\mathbf{w}$** is a trainable parameter.\n",
    "    - **$c$** is a fixed constant **(not updated during optimization)**.\n",
    "\n",
    "- We implement this idea in the `FixedHiddenMLP` class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f8721f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:31:40.334075Z",
     "iopub.status.busy": "2023-08-18T19:31:40.333497Z",
     "iopub.status.idle": "2023-08-18T19:31:40.340281Z",
     "shell.execute_reply": "2023-08-18T19:31:40.339397Z"
    },
    "origin_pos": 36,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Random weight parameters that will not compute gradients and\n",
    "        # therefore keep constant during training\n",
    "        self.rand_weight = torch.rand((20, 20))\n",
    "        self.linear = nn.LazyLinear(20)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.linear(X)\n",
    "        X = F.relu(X @ self.rand_weight + 1)\n",
    "        # Reuse the fully connected layer. This is equivalent to sharing\n",
    "        # parameters with two fully connected layers\n",
    "        X = self.linear(X)\n",
    "        # Control flow\n",
    "        while X.abs().sum() > 1:\n",
    "            X /= 2\n",
    "        return X.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e65b0b",
   "metadata": {
    "origin_pos": 39
   },
   "source": [
    "- In this model, we implement a **hidden layer with fixed weights**:\n",
    "  - `self.rand_weight` is initialized **randomly** at instantiation.\n",
    "  - These weights **remain constant** thereafter.\n",
    "  - This weight **is not a model parameter** and is **never updated by backpropagation**.\n",
    "- The network structure:\n",
    "  1. The **output of the fixed layer** is passed through a **fully connected layer**.\n",
    "\n",
    "- **Unusual operation before returning the output**:\n",
    "  1. **A while-loop runs**, checking if the **$\\ell_1$ norm** of the output is **greater than 1**.\n",
    "  2. If the condition holds:\n",
    "     - The output vector is **divided by 2** until it satisfies the condition.\n",
    "  3. The **sum of the entries in `X`** is returned.\n",
    "\n",
    "- **Key observations**:\n",
    "  - No standard neural network **performs this type of operation**.\n",
    "  - This particular operation **may not be useful** in real-world tasks.\n",
    "  - **The purpose** of this implementation:\n",
    "    - To demonstrate how to **integrate arbitrary code** into a neural network's computation flow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ede5347f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:31:40.344398Z",
     "iopub.status.busy": "2023-08-18T19:31:40.343674Z",
     "iopub.status.idle": "2023-08-18T19:31:40.355810Z",
     "shell.execute_reply": "2023-08-18T19:31:40.353856Z"
    },
    "origin_pos": 40,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.3836, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = FixedHiddenMLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e343c1a3",
   "metadata": {
    "origin_pos": 42
   },
   "source": [
    "- We can **mix and match various ways** of assembling modules together.\n",
    "- In the following example, we **nest modules** in creative ways.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0c3d190",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:31:40.359588Z",
     "iopub.status.busy": "2023-08-18T19:31:40.359258Z",
     "iopub.status.idle": "2023-08-18T19:31:40.372492Z",
     "shell.execute_reply": "2023-08-18T19:31:40.371497Z"
    },
    "origin_pos": 44,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0679, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.LazyLinear(64), nn.ReLU(),\n",
    "                                 nn.LazyLinear(32), nn.ReLU())\n",
    "        self.linear = nn.LazyLinear(16)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.linear(self.net(X))\n",
    "\n",
    "chimera = nn.Sequential(NestMLP(), nn.LazyLinear(20), FixedHiddenMLP())\n",
    "chimera(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48005bbf",
   "metadata": {
    "origin_pos": 47
   },
   "source": [
    "## **Summary**\n",
    "\n",
    "- **Modules and Layers**:\n",
    "  - **Individual layers** can be **modules**.\n",
    "  - **Many layers** can comprise a **module**.\n",
    "  - **Many modules** can comprise a **module**.\n",
    "\n",
    "- **Capabilities of a Module**:\n",
    "  - A module **can contain code** beyond standard layers.\n",
    "  - Modules **handle housekeeping tasks**, including:\n",
    "    - **Parameter initialization**.\n",
    "    - **Backpropagation**.\n",
    "\n",
    "- **Sequential Module**:\n",
    "  - Handles **sequential concatenation** of layers and modules.\n",
    "  - Enables **building complex architectures** with minimal effort.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "required_libs": []
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
